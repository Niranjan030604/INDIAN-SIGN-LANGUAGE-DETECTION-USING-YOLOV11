# INDIAN SIGN LANGUAGE DETECTION USING YOLOV11
This repository contains a real-time Indian Sign Language Detection application developed using the YOLOv11 model. The goal of the project is to make sign language communication more accessible by converting hand gestures into meaningful labels using deep learning.  ðŸ”¹ Key Features  Real-time ISL gesture recognition  YOLOv11-based object detection  Custom dataset training pipeline  Camera-based live inference  Dataset annotation via CVAT/Roboflow (optional)  Accuracy & mAP evaluation support  Fast inference optimized for GPU/CPU  ðŸ”¹ Technical Workflow  Dataset collection & annotation (bounding boxes for gestures)  Data preprocessing & conversion to YOLO format  Model training using YOLOv11  Validation and mAP evaluation  Real-time inference using webcam  Output gesture class labels on screen  ðŸ”¹ Tech Stack Component	Tools Model	YOLOv11 Framework	PyTorch Language	Python Dataset	Custom ISL Gesture Dataset Tools Used	OpenCV, Roboflow/CVAT, NumPy Inference	OpenCV + PyTorch runtime ðŸ”¹ Use Cases  Assistive communication for hearing-impaired users  Gesture-based interfaces  Educational tools for learning ISL  Research in sign language recognition
